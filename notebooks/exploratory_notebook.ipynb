# Exploratory Data Analysis for Country Name Classification
# This notebook explores the dataset and helps understand patterns

# %% [markdown]
# # Country Name Classification - Exploratory Data Analysis
# 
# This notebook performs:
# 1. Data loading and initial exploration
# 2. Country name pattern analysis
# 3. Negative example (non-country) pattern analysis
# 4. Feature importance analysis
# 5. Data quality checks
# 6. Class balance analysis

# %% Setup
import sys
sys.path.append('..')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from wordcloud import WordCloud
import warnings
warnings.filterwarnings('ignore')

# Set style
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (14, 6)

# %% [markdown]
# ## 1. Load and Inspect Data

# %%
# Load the processed datasets
try:
    train_df = pd.read_csv('../data/processed/train.csv')
    val_df = pd.read_csv('../data/processed/val.csv')
    test_df = pd.read_csv('../data/processed/test.csv')
    
    print("✓ Data loaded successfully!")
    print(f"Train set: {len(train_df)} samples")
    print(f"Validation set: {len(val_df)} samples")
    print(f"Test set: {len(test_df)} samples")
except FileNotFoundError:
    print("⚠ Processed data not found. Please run data preparation first:")
    print("  python main.py --mode prepare --csv data/raw/country_aliases.csv")
    train_df = pd.DataFrame()

# %%
# Display first few rows
if not train_df.empty:
    print("\nFirst 10 samples:")
    print(train_df.head(10))
    
    print("\nDataset Info:")
    print(train_df.info())
    
    print("\nBasic Statistics:")
    print(train_df.describe())

# %% [markdown]
# ## 2. Class Distribution Analysis

# %%
if not train_df.empty:
    # Combine all datasets for complete analysis
    all_data = pd.concat([train_df, val_df, test_df], ignore_index=True)
    
    # Class distribution
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Overall distribution
    class_counts = all_data['label'].value_counts()
    axes[0].bar(['Not Country', 'Country'], class_counts.values, 
                color=['#e74c3c', '#2ecc71'])
    axes[0].set_title('Overall Class Distribution', fontweight='bold')
    axes[0].set_ylabel('Count')
    for i, v in enumerate(class_counts.values):
        axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')
    
    # Pie chart
    axes[1].pie(class_counts.values, labels=['Not Country', 'Country'], 
                autopct='%1.1f%%', colors=['#e74c3c', '#2ecc71'], 
                startangle=90)
    axes[1].set_title('Class Proportion', fontweight='bold')
    
    # Distribution by dataset
    dataset_dist = pd.DataFrame({
        'Train': train_df['label'].value_counts(),
        'Val': val_df['label'].value_counts(),
        'Test': test_df['label'].value_counts()
    }).T
    dataset_dist.columns = ['Not Country', 'Country']
    dataset_dist.plot(kind='bar', ax=axes[2], color=['#e74c3c', '#2ecc71'])
    axes[2].set_title('Class Distribution by Dataset', fontweight='bold')
    axes[2].set_ylabel('Count')
    axes[2].set_xlabel('Dataset')
    axes[2].legend(title='Class')
    axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=0)
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nClass Balance Ratio: {class_counts[1] / class_counts[0]:.2f}")

# %% [markdown]
# ## 3. Text Length Analysis

# %%
if not train_df.empty:
    # Calculate text lengths
    all_data['text_length'] = all_data['text'].str.len()
    all_data['word_count'] = all_data['text'].str.split().str.len()
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Text length by class - Histogram
    for label, name in [(0, 'Not Country'), (1, 'Country')]:
        data = all_data[all_data['label'] == label]['text_length']
        color = '#e74c3c' if label == 0 else '#2ecc71'
        axes[0, 0].hist(data, bins=30, alpha=0.6, label=name, color=color, edgecolor='black')
    axes[0, 0].set_xlabel('Text Length (characters)')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title('Text Length Distribution by Class', fontweight='bold')
    axes[0, 0].legend()
    axes[0, 0].grid(alpha=0.3)
    
    # Word count by class - Histogram
    for label, name in [(0, 'Not Country'), (1, 'Country')]:
        data = all_data[all_data['label'] == label]['word_count']
        color = '#e74c3c' if label == 0 else '#2ecc71'
        axes[0, 1].hist(data, bins=20, alpha=0.6, label=name, color=color, edgecolor='black')
    axes[0, 1].set_xlabel('Word Count')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].set_title('Word Count Distribution by Class', fontweight='bold')
    axes[0, 1].legend()
    axes[0, 1].grid(alpha=0.3)
    
    # Box plots
    all_data.boxplot(column='text_length', by='label', ax=axes[1, 0])
    axes[1, 0].set_title('Text Length by Class', fontweight='bold')
    axes[1, 0].set_xlabel('Class (0=Not Country, 1=Country)')
    axes[1, 0].set_ylabel('Text Length')
    plt.suptitle('')
    
    all_data.boxplot(column='word_count', by='label', ax=axes[1, 1])
    axes[1, 1].set_title('Word Count by Class', fontweight='bold')
    axes[1, 1].set_xlabel('Class (0=Not Country, 1=Country)')
    axes[1, 1].set_ylabel('Word Count')
    plt.suptitle('')
    
    plt.tight_layout()
    plt.show()
    
    # Statistics
    print("\nText Length Statistics by Class:")
    print(all_data.groupby('label')['text_length'].describe())
    
    print("\nWord Count Statistics by Class:")
    print(all_data.groupby('label')['word_count'].describe())

# %% [markdown]
# ## 4. Common Words and Patterns

# %%
if not train_df.empty:
    # Extract common words for each class
    def get_word_freq(texts, top_n=20):
        all_words = ' '.join(texts).lower().split()
        return Counter(all_words).most_common(top_n)
    
    # Countries
    country_texts = all_data[all_data['label'] == 1]['text']
    country_words = get_word_freq(country_texts.tolist())
    
    # Non-countries
    non_country_texts = all_data[all_data['label'] == 0]['text']
    non_country_words = get_word_freq(non_country_texts.tolist())
    
    # Plot
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Country words
    words, counts = zip(*country_words)
    axes[0].barh(words, counts, color='#2ecc71', edgecolor='black')
    axes[0].set_xlabel('Frequency')
    axes[0].set_title('Top 20 Words in Country Names', fontweight='bold')
    axes[0].invert_yaxis()
    
    # Non-country words
    words, counts = zip(*non_country_words)
    axes[1].barh(words, counts, color='#e74c3c', edgecolor='black')
    axes[1].set_xlabel('Frequency')
    axes[1].set_title('Top 20 Words in Non-Country Names', fontweight='bold')
    axes[1].invert_yaxis()
    
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 5. Word Clouds

# %%
if not train_df.empty:
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Country word cloud
    country_text = ' '.join(all_data[all_data['label'] == 1]['text'].tolist())
    wordcloud_country = WordCloud(
        width=800, height=400, 
        background_color='white',
        colormap='Greens'
    ).generate(country_text)
    
    axes[0].imshow(wordcloud_country, interpolation='bilinear')
    axes[0].axis('off')
    axes[0].set_title('Country Names Word Cloud', fontsize=14, fontweight='bold')
    
    # Non-country word cloud
    non_country_text = ' '.join(all_data[all_data['label'] == 0]['text'].tolist())
    wordcloud_non = WordCloud(
        width=800, height=400,
        background_color='white',
        colormap='Reds'
    ).generate(non_country_text)
    
    axes[1].imshow(wordcloud_non, interpolation='bilinear')
    axes[1].axis('off')
    axes[1].set_title('Non-Country Names Word Cloud', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 6. Pattern Analysis

# %%
if not train_df.empty:
    # Define patterns to search for
    patterns = {
        'Republic': r'\brepublic\b',
        'Kingdom': r'\bkingdom\b',
        'State': r'\bstate\b',
        'Bank': r'\bbank\b',
        'University': r'\buniversity\b',
        'Hotel': r'\bhotel\b',
        'Airlines': r'\bairline',
        'Embassy': r'\bembassy\b',
        'Company': r'\b(inc|llc|ltd|corp|company)\b',
    }
    
    # Count patterns in each class
    pattern_counts = {'Country': {}, 'Not Country': {}}
    
    for pattern_name, pattern_regex in patterns.items():
        for label, class_name in [(1, 'Country'), (0, 'Not Country')]:
            texts = all_data[all_data['label'] == label]['text']
            count = texts.str.contains(pattern_regex, case=False, regex=True).sum()
            pattern_counts[class_name][pattern_name] = count
    
    # Plot
    df_patterns = pd.DataFrame(pattern_counts)
    
    fig, ax = plt.subplots(figsize=(12, 6))
    df_patterns.plot(kind='bar', ax=ax, color=['#2ecc71', '#e74c3c'], 
                     edgecolor='black', width=0.8)
    ax.set_xlabel('Pattern', fontsize=12)
    ax.set_ylabel('Frequency', fontsize=12)
    ax.set_title('Pattern Frequency by Class', fontsize=14, fontweight='bold')
    ax.legend(title='Class')
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    ax.grid(alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.show()
    
    print("\nPattern Frequency Table:")
    print(df_patterns)

# %% [markdown]
# ## 7. Capitalization Analysis

# %%
if not train_df.empty:
    # Analyze capitalization patterns
    def analyze_caps(text):
        words = text.split()
        if not words:
            return 0
        capitalized = sum(1 for word in words if word and word[0].isupper())
        return capitalized / len(words)
    
    all_data['cap_ratio'] = all_data['text'].apply(analyze_caps)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Histogram
    for label, name in [(0, 'Not Country'), (1, 'Country')]:
        data = all_data[all_data['label'] == label]['cap_ratio']
        color = '#e74c3c' if label == 0 else '#2ecc71'
        axes[0].hist(data, bins=20, alpha=0.6, label=name, color=color, edgecolor='black')
    axes[0].set_xlabel('Capitalization Ratio')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('Capitalization Pattern by Class', fontweight='bold')
    axes[0].legend()
    axes[0].grid(alpha=0.3)
    
    # Box plot
    all_data.boxplot(column='cap_ratio', by='label', ax=axes[1])
    axes[1].set_title('Capitalization Ratio by Class', fontweight='bold')
    axes[1].set_xlabel('Class (0=Not Country, 1=Country)')
    axes[1].set_ylabel('Capitalization Ratio')
    plt.suptitle('')
    
    plt.tight_layout()
    plt.show()
    
    print("\nCapitalization Statistics by Class:")
    print(all_data.groupby('label')['cap_ratio'].describe())

# %% [markdown]
# ## 8. Sample Examples from Each Class

# %%
if not train_df.empty:
    print("\n" + "="*70)
    print("SAMPLE EXAMPLES FROM EACH CLASS")
    print("="*70)
    
    print("\n📍 COUNTRY EXAMPLES (label=1):")
    print("-"*70)
    country_samples = all_data[all_data['label'] == 1]['text'].sample(min(15, len(all_data[all_data['label'] == 1])))
    for i, text in enumerate(country_samples, 1):
        print(f"{i:2d}. {text}")
    
    print("\n🏢 NON-COUNTRY EXAMPLES (label=0):")
    print("-"*70)
    non_country_samples = all_data[all_data['label'] == 0]['text'].sample(min(15, len(all_data[all_data['label'] == 0])))
    for i, text in enumerate(non_country_samples, 1):
        print(f"{i:2d}. {text}")

# %% [markdown]
# ## 9. Data Quality Checks

# %%
if not train_df.empty:
    print("\n" + "="*70)
    print("DATA QUALITY CHECKS")
    print("="*70)
    
    # Check for missing values
    print("\n1. Missing Values:")
    print(all_data.isnull().sum())
    
    # Check for duplicates
    print("\n2. Duplicate Entries:")
    duplicates = all_data.duplicated(subset=['text']).sum()
    print(f"   Total duplicates: {duplicates}")
    if duplicates > 0:
        print("\n   Sample duplicates:")
        dup_texts = all_data[all_data.duplicated(subset=['text'], keep=False)].sort_values('text')
        print(dup_texts.head(10))
    
    # Check for empty strings
    print("\n3. Empty or Very Short Texts:")
    short_texts = all_data[all_data['text'].str.len() < 3]
    print(f"   Texts with < 3 characters: {len(short_texts)}")
    if len(short_texts) > 0:
        print(short_texts)
    
    # Check for very long texts (potential data issues)
    print("\n4. Unusually Long Texts:")
    long_texts = all_data[all_data['text'].str.len() > 100]
    print(f"   Texts with > 100 characters: {len(long_texts)}")
    if len(long_texts) > 0:
        print(long_texts[['text', 'label']].head())

# %% [markdown]
# ## 10. Feature Correlation Analysis (Numerical Features)

# %%
if not train_df.empty:
    # Create some numerical features for correlation analysis
    feature_df = pd.DataFrame({
        'label': all_data['label'],
        'text_length': all_data['text_length'],
        'word_count': all_data['word_count'],
        'cap_ratio': all_data['cap_ratio'],
        'has_of': all_data['text'].str.contains(' of ', case=False).astype(int),
        'has_the': all_data['text'].str.lower().str.startswith('the ').astype(int),
        'has_republic': all_data['text'].str.contains('republic', case=False).astype(int),
        'has_kingdom': all_data['text'].str.contains('kingdom', case=False).astype(int),
        'has_bank': all_data['text'].str.contains('bank', case=False).astype(int),
        'has_university': all_data['text'].str.contains('university', case=False).astype(int),
    })
    
    # Correlation matrix
    corr_matrix = feature_df.corr()
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
                center=0, square=True, linewidths=1)
    plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    # Correlation with target
    print("\nCorrelation with Target (label):")
    print(corr_matrix['label'].sort_values(ascending=False))

# %% [markdown]
# ## 11. N-gram Analysis

# %%
if not train_df.empty:
    from sklearn.feature_extraction.text import CountVectorizer
    
    # Bigram analysis for countries
    print("\n" + "="*70)
    print("TOP BIGRAMS IN COUNTRY NAMES")
    print("="*70)
    
    vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=20)
    country_bigrams = vectorizer.fit_transform(all_data[all_data['label'] == 1]['text'])
    bigram_freq = dict(zip(vectorizer.get_feature_names_out(), 
                          country_bigrams.toarray().sum(axis=0)))
    
    sorted_bigrams = sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True)
    
    for i, (bigram, freq) in enumerate(sorted_bigrams[:15], 1):
        print(f"{i:2d}. '{bigram}' - {freq} occurrences")
    
    # Bigram analysis for non-countries
    print("\n" + "="*70)
    print("TOP BIGRAMS IN NON-COUNTRY NAMES")
    print("="*70)
    
    vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=20)
    non_country_bigrams = vectorizer.fit_transform(all_data[all_data['label'] == 0]['text'])
    bigram_freq = dict(zip(vectorizer.get_feature_names_out(), 
                          non_country_bigrams.toarray().sum(axis=0)))
    
    sorted_bigrams = sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True)
    
    for i, (bigram, freq) in enumerate(sorted_bigrams[:15], 1):
        print(f"{i:2d}. '{bigram}' - {freq} occurrences")

# %% [markdown]
# ## 12. Character-level Analysis

# %%
if not train_df.empty:
    # Analyze character patterns
    def get_char_freq(texts, top_n=15):
        all_chars = ''.join(texts).lower()
        # Only keep letters
        all_chars = ''.join(c for c in all_chars if c.isalpha())
        return Counter(all_chars).most_common(top_n)
    
    country_chars = get_char_freq(all_data[all_data['label'] == 1]['text'].tolist())
    non_country_chars = get_char_freq(all_data[all_data['label'] == 0]['text'].tolist())
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 5))
    
    # Country characters
    chars, counts = zip(*country_chars)
    axes[0].bar(chars, counts, color='#2ecc71', edgecolor='black')
    axes[0].set_xlabel('Character')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('Top Characters in Country Names', fontweight='bold')
    
    # Non-country characters
    chars, counts = zip(*non_country_chars)
    axes[1].bar(chars, counts, color='#e74c3c', edgecolor='black')
    axes[1].set_xlabel('Character')
    axes[1].set_ylabel('Frequency')
    axes[1].set_title('Top Characters in Non-Country Names', fontweight='bold')
    
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## Summary and Insights

# %%
if not train_df.empty:
    print("\n" + "="*70)
    print("KEY INSIGHTS FROM EXPLORATORY ANALYSIS")
    print("="*70)
    
    print("\n1. DATA OVERVIEW:")
    print(f"   • Total samples: {len(all_data):,}")
    print(f"   • Country names: {(all_data['label'] == 1).sum():,}")
    print(f"   • Non-country names: {(all_data['label'] == 0).sum():,}")
    print(f"   • Class balance ratio: {(all_data['label'] == 1).sum() / (all_data['label'] == 0).sum():.2f}")
    
    print("\n2. TEXT CHARACTERISTICS:")
    country_avg_len = all_data[all_data['label'] == 1]['text_length'].mean()
    non_country_avg_len = all_data[all_data['label'] == 0]['text_length'].mean()
    print(f"   • Avg country name length: {country_avg_len:.1f} characters")
    print(f"   • Avg non-country name length: {non_country_avg_len:.1f} characters")
    
    country_avg_words = all_data[all_data['label'] == 1]['word_count'].mean()
    non_country_avg_words = all_data[all_data['label'] == 0]['word_count'].mean()
    print(f"   • Avg country name words: {country_avg_words:.1f}")
    print(f"   • Avg non-country name words: {non_country_avg_words:.1f}")
    
    print("\n3. KEY DISTINGUISHING PATTERNS:")
    print("   Country indicators:")
    print("     - 'republic', 'kingdom', 'state', 'federation'")
    print("     - Higher capitalization ratio")
    print("     - Shorter text length on average")
    
    print("\n   Non-country indicators:")
    print("     - 'bank', 'university', 'hotel', 'airlines'")
    print("     - 'inc', 'ltd', 'llc', 'corp'")
    print("     - Organization-specific terms")
    
    print("\n4. DATA QUALITY:")
    print(f"   • Duplicates: {all_data.duplicated(subset=['text']).sum()}")
    print(f"   • Missing values: {all_data.isnull().sum().sum()}")
    print(f"   • Empty texts: {(all_data['text'].str.len() < 3).sum()}")
    
    print("\n5. RECOMMENDATIONS:")
    print("   ✓ Use character n-grams (1-3) for better pattern matching")
    print("   ✓ Include keyword-based features (republic, bank, etc.)")
    print("   ✓ Use capitalization ratio as a feature")
    print("   ✓ Text length features can help distinguish classes")
    print("   ✓ TF-IDF vectorization should capture key terms well")

else:
    print("\n⚠ No data available for analysis")
    print("Run data preparation first: python main.py --mode prepare --csv data/raw/country_aliases.csv")

# %% [markdown]
# ## Next Steps
# 
# Based on this EDA, the next steps are:
# 1. ✅ Feature engineering incorporating observed patterns
# 2. ✅ Model training with multiple algorithms
# 3. ✅ Hyperparameter tuning
# 4. ✅ Model evaluation and error analysis
# 5. ✅ Deploy the best model
# 
# The patterns discovered here will inform our feature engineering strategy!